{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual data\n",
    "\n",
    "##  Reading text from files\n",
    "\n",
    "We use python 3.4 to read text from two files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = open('abipon-latin1.txt', 'rb').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Abip\\xf3n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2 = open('abipon-utf8.txt', 'rb').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Abipo\\xcc\\x81n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary content of the two files is obviously different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 == s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s1) == len(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to decode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf3 in position 4: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0a4621444d34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf3 in position 4: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "s1.decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding as UTF-8 did not work. Let's try a different common encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abipón'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.decode('latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. That looks about right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AbipoÌ\\x81n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2.decode('latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abipón'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2.decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! So the content of the two files may be the same after all!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Textual data may be represented in files in different [character encodings](https://en.wikipedia.org/wiki/Character_encoding).\n",
    "- Common encodings are \n",
    "  - [ASCII](https://en.wikipedia.org/wiki/ASCII)\n",
    "  - Latin 1, aka [ISO 8859-1](https://en.wikipedia.org/wiki/ISO/IEC_8859-1) Western Europe\n",
    "  - CP-1252, aka [Windows-1252](https://en.wikipedia.org/wiki/Windows-1252)\n",
    "  - macroman, aka [Mac OS Roman](https://en.wikipedia.org/wiki/Mac_OS_Roman)\n",
    "  - [UTF-8](https://en.wikipedia.org/wiki/UTF-8)\n",
    "- All the above encodings are compatible with ASCII, i.e. ASCII-only text will be encoded\n",
    "  identically in all these encodings.\n",
    "- But generally comparing text in different encodings does not make sense.\n",
    "- UTF-8 is the only Unicode encoding of the ones listed above, i.e. it can encode all characters\n",
    "  defined in the [Unicode standard](https://en.wikipedia.org/wiki/Unicode).\n",
    "- It is impossible in general to automatically detect the encoding used in a file, since\n",
    "  for example everything can be \"decoded\" as Latin 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation of text in software: Unicode\n",
    "\n",
    "Software systems need a unified internal representation of textual data. This is typically\n",
    "either a unicode encoding like UTF-8 (R) or UTF-16 (Windows) or an implementation of\n",
    "Unicode (Python).\n",
    "\n",
    "The advantages of implementing the Unicode standard are:\n",
    "- character properties:\n",
    "  - category (letter [uppercase|lowercase|...], punctuation, symbol, ...)\n",
    "  - script (Coptic, Cyrillic, ...)\n",
    "- rules for normalization, decomposition, collation, bidirectional display\n",
    "\n",
    "So let's see how our data looks internally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = s1.decode('latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2 = s2.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 == s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm. So this is not WYSIWYG. Looking the same is not enough to pass as identity in Unicode. There are many instances where the fonts rendering the glyphs for different sequences of unicode code points will look the same, because\n",
    "- different scripts may have characters looking alike, e.g. the cyrillic letter а and the latin letter a\n",
    "- there are different unicode sequences representing the same glyphs, e.g. through the use of combining accents or diacritics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s1), len(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find out what's going on here with the help of the [unicodedata](https://docs.python.org/3/library/unicodedata.html) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATIN CAPITAL LETTER A\n",
      "LATIN SMALL LETTER B\n",
      "LATIN SMALL LETTER I\n",
      "LATIN SMALL LETTER P\n",
      "LATIN SMALL LETTER O WITH ACUTE\n",
      "LATIN SMALL LETTER N\n"
     ]
    }
   ],
   "source": [
    "for c in s1:\n",
    "    print(unicodedata.name(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATIN CAPITAL LETTER A\n",
      "LATIN SMALL LETTER B\n",
      "LATIN SMALL LETTER I\n",
      "LATIN SMALL LETTER P\n",
      "LATIN SMALL LETTER O\n",
      "COMBINING ACUTE ACCENT\n",
      "LATIN SMALL LETTER N\n"
     ]
    }
   ],
   "source": [
    "for c in s2:\n",
    "    print(unicodedata.name(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the difference comes from the fact that in `s2`, the ó has been composed from two unicode code points. Fortunately unicode has a concept of [equivalence](https://en.wikipedia.org/wiki/Unicode_equivalence) which solves this problem, and [normalization](https://en.wikipedia.org/wiki/Unicode_equivalence#Normalization) is the way to assess equivalence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either force the canonical composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 == unicodedata.normalize('NFC', s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or canonical decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFD', s1) == s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- Naive string comparisons do not work when text is encoded differently.\n",
    "- It is also not that easy when text is encoded with the same encoding ...\n",
    "- ... and it is not easy with unicode either.\n",
    "- But unicode provides the tools (normalization!) to make it work.\n",
    "- The unicode support in python does not mean normalization is applied implicitely before comparing strings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## More pitfalls\n",
    "\n",
    "### Line endings\n",
    "\n",
    "Quoting [Wikipedia](https://en.wikipedia.org/wiki/Newline)\n",
    "\n",
    "> Systems based on ASCII or a compatible character set use **either** LF (Line feed, '\\n') **or** CR (Carriage return, '\\r') individually, **or** CR followed by LF (CR+LF, '\\r\\n').\n",
    "\n",
    "Or to quote [Jenny Bryan](https://gist.github.com/jennybc)\n",
    "\n",
    "| English                             | OS connotation                     | character | \"vibe\"        |\n",
    "|-------------------------------------|------------------------------------|-----------|---------------|\n",
    "| carriage return, \"CR\"               | classic Mac, i.e. OS 9 and earlier | `\\r`      | archaic       |\n",
    "| line feed, \"LF\"                     | Unix, including Mac OS X           | `\\n`      | The Very Best |\n",
    "| carriage return + line feed, \"CRLF\" | Windows, going back to DOS         | `\\r\\n`    | Boo! Windows! |\n",
    "\n",
    "CSV files exported from LibreOffice on Linux will have LF as line endings, if exported from [Excel on Mac OS](https://gist.github.com/jennybc/0be7717c2b5b30088811), it will be CR, if exported from Excel on Windows it will be CR+LF.\n",
    "\n",
    "So it is quite likely that you will run into different line endings at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's [`open` builtin function](https://docs.python.org/3/library/functions.html#open) supports opening files in [universal newlines](https://docs.python.org/3/glossary.html#term-universal-newlines) mode, by passing `newline=None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('newlines.txt') as fp:\n",
    "    lines = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['line\\n', 'line\\n', 'line\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('newlines.txt', newline='') as fp:\n",
    "    lines = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['line\\n', 'line\\r', 'line\\r\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('newlines.txt', newline='\\r') as fp:\n",
    "    lines = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['line\\nline\\r', 'line\\r', '\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[git can help](https://help.github.com/articles/dealing-with-line-endings/) to solve this problem in a collaborative setting, by [configuring what line endings to use for text files in a repository](https://help.github.com/articles/dealing-with-line-endings/#per-repository-settings).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control characters\n",
    "\n",
    "Why are things like LF and CR in ASCII anyway, rather than just NEWLINE? I guess I'm too young to know.\n",
    "\n",
    "But there are more weird things in ASCII:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = '\\x07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't see them. But they must have properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no such name",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3bd6f1f8ab28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0municodedata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: no such name"
     ]
    }
   ],
   "source": [
    "unicodedata.name(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm. That's diappointing. No name for these things? [It turns out](http://stackoverflow.com/a/24553272) the generic [name for these is just `<control>`](http://www.unicode.org/Public/5.2.0/ucd/UnicodeData.txt), although this specific character is called *BELL*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cc\n"
     ]
    }
   ],
   "source": [
    "print(unicodedata.category(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looking this up in [the specification](http://www.unicode.org/reports/tr44/#GC_Values_Table) reveals this is in fact a control character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then, there's all kinds of funny stuff in Unicode, and an encoding like UTF-8 can handle all of it, so putting them in text files shouldn't be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ExpatError",
     "evalue": "not well-formed (invalid token): line 1, column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExpatError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-77c30dc45a3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mminidom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<e>'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'</e>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/python3.4/xml/dom/minidom.py\u001b[0m in \u001b[0;36mparseString\u001b[1;34m(string, parser)\u001b[0m\n\u001b[0;32m   1968\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1970\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1971\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1972\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpulldom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/xml/dom/expatbuilder.py\u001b[0m in \u001b[0;36mparseString\u001b[1;34m(string, namespaces)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m         \u001b[0mbuilder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExpatBuilder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/xml/dom/expatbuilder.py\u001b[0m in \u001b[0;36mparseString\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mParseEscape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExpatError\u001b[0m: not well-formed (invalid token): line 1, column 3"
     ]
    }
   ],
   "source": [
    "minidom.parseString('<e>' + s + '</e>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch! I turns out, while the default character encoding for XML files is UTF-8, actually [not all of UTF-8 is valid](https://www.w3.org/TR/xml/#charsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you want to put UTF-8 encoded data into a [BEAST2](http://beast2.org/) XML configuration file, you may have to remove the control characters first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encountering control characters in text files is not uncommon, in particular of the files are old and/or composed by copying and pasting text from different applications on different platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
